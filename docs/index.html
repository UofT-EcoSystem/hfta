
<!doctype html>
<html>

<head>
    <title>IOS: Inter-Operator Scheduler for CNN Acceleration</title>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-150911566-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-150911566-1');
    </script>

    <meta name="viewport" content="width=device-width,initial-scale=1">
    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-3.2.1.min.js"
            integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link href="css/style.css" rel="stylesheet">

    <style>
        .paperthumb {
            float: left;
            width: 120px;
            margin: 3px 10px 7px 0;
        }

        .paperdesc {
            clear: both;
        }
    </style>
</head>

<body class="nd-docs">
<div class="nd-pageheader">
    <div class="container">
        <p class="lead">
        <p style="font-size:36px"><b>IOS: Inter-Operator Scheduler for CNN Acceleration</b></p>
        <br>
        <address>
            <!--     <nobr><a href="https://yaoyaoding.com/">Yaoyao Ding</a><sup>*,1,2</sup>,</nobr>-->
            <nobr><a href="https://www.yaoyaoding.com/" target="_blank">Yaoyao Ding</a><sup>1,2</sup>, </nobr>
            <nobr><a href="https://lzhu.me/" target="_blank">Ligeng Zhu</a><sup>3</sup>, </nobr>
            <nobr><a href="https://cs.stanford.edu/~zhihao/" target="_blank">Zhihao Jia</a><sup>4</sup>, </nobr>
            <nobr><a href="http://www.cs.toronto.edu/~pekhimenko/" target="_blank">Gennady Pekhimenko</a><sup>1,2</sup>, </nobr>
            <nobr><a href="https://songhan.mit.edu/" target="_blank">Song Han</a><sup>3</sup></nobr>
            <br>
            <nobr><sup>1</sup>University of Toronto,</nobr>
            <nobr><sup>2</sup>Vector Institute,</nobr>
            <nobr><sup>3</sup>Massachusetts Institute of Technology,</nobr>
            <nobr><sup>4</sup>Carnegie Mellon University</nobr>
            <br>
            <br>
            <nobr> In <a href="https://mlsys.org/" target="_blank">Proceedings of Machine Learning and Systems 3 (MLSys 2021)</a></nobr>
<!--            <br>-->
<!--            <br>-->
<!--            <nobr>This website is under <strong>construction</strong></nobr>-->

            <!--     <br>-->
            <!--     <nobr><sup>*</sup>Work done w internship at MIT-HAN lab.</nobr>-->
        </address>
        </p>
    </div>
</div> <!-- end nd-pageheader -->


<div class="container">

    <div class="row">
        <div class="col text-center">
            <p>
                <a href="https://proceedings.mlsys.org/paper/2021/file/38b3eff8baf56627478ec76a704e9b52-Paper.pdf" class="d-inline-block p-3">
                    <img height="100" src="figures/paper-thumbnail.png" style="border:1px solid" data-nothumb>
                    <br>
                    <i class="fas fa-file-alt"> Paper</i>
                </a>
                <a href="files/mlsys-ios-slides.pdf" class="d-inline-block p-3">
                    <!--         <img src="figures/slides-thumbnail.png" style="border:1px solid; margin-bottom:16px" height="100" data-nothumb>-->
                    <img src="figures/slides-thumbnail.png" style="border:1px solid" height="100" data-nothumb>
                    <br>
                    <i class="fas fa-file-powerpoint"> Slides</i>
                </a>
                <a href="files/mlsys-ios-poster.pdf" class="d-inline-block p-3">
                    <img height="100" src="figures/poster-thumbnail.png" style="border:1px solid" data-nothumb>
                    <br>
                    <i class="fas fa-columns"> Poster</i>
                </a>
                <a href="https://github.com/mit-han-lab/inter-operator-scheduler" class="d-inline-block p-3">
                    <img height="100" src="figures/github-thumbnail.png" style="border:1px solid" data-nothumb>
                    <br>
                    <i class="fab fa-github-square"> <b>GitHub</b></i>
                </a>
            </p>
        </div>
    </div>

    <hr />

    <div class="row">
        <div class="col">
            <p>
                <strong>Abstract:</strong> To accelerate CNN inference, existing deep learning frameworks focus on optimizing <strong>intra-operator
                parallelization</strong>. However, a single operator can no longer fully utilize the available parallelism given the rapid advances
                in high-performance hardware, resulting in a large gap between the peak performance and the real performance.
                This performance gap is more severe under smaller batch sizes. In this work, we extensively study the
                <strong>parallelism between operators</strong> and propose <strong>Inter-Operator Scheduler (IOS)</strong> to automatically schedule multiple
                operators' parallel execution through a novel dynamic programming algorithm. IOS consistently outperforms
                state-of-the-art libraries (e.g., TensorRT) by <strong>1.1 to 1.5x</strong> on modern CNN benchmarks.
            </p>
            <h2>Introduction</h2>
<!--            <center class="embed-responsive embed-responsive-16by9">-->
            <center class="embed-responsive-16by9">
                <iframe width="1080" height="607.5" src="https://www.youtube.com/embed/Bsucxo3Yils" title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </center>
            For the full talk at MLSys'21, please refer to <a target="_blank" href="https://youtu.be/js7ov6PQA7E">here</a>.
            <!--            <center class="embed-responsive embed-responsive-16by9">-->
            <!--                <iframe class="embed-responsive-item" width="560" height="315" src="https://www.youtube.com/embed/h6zbQe5U1v4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
            <!--            </center>-->

            <h2>Inter-Operator Scheduler (IOS)</h2>
            <p style="text-align: center">
                <img src="figures/ios-overview.png" style="max-width:75%">
                <br>
                Overview of Inter-Operator Scheduler
            </p>
            IOS explores the inter-operator parallelization schedule space <strong>exhaustively</strong> through <strong>dynamic programming</strong> algorithm.
            <!--            It reuses the optimal schedule for each sub-graph.-->
            <br>
            For each subset of operators S (initially, S contains all operators in the model):
            <ol>
                <li>Enumerate a subset S' of S, taking it as the <i>last stage</i> of S.</li>
                <li>Convert the original problem that finds the optimal schedule for S, into a sub-problem that finds the optimal schedule for S-S'.</li>
            </ol>
            IOS would record the optimal choice of S' for each S and use it to construct the optimal schedule. IOS reuses the optimal schedule for each subgraph.
            <br>
            We prove that its <strong>time complexity</strong> is \( (n/d+1)^{2d} \), where \(n\) is the number of operators and \(d\) is the maximum number of parallelizable operators.
            The time complexity is only exponential in \(d\), that is usually small, making it feasible to explore the schedule space exhasutively.

            <h2>Parallelization Strategies in IOS</h2>
            <p style="text-align: center">
                <img src="figures/strategies.png" style="max-width:75%">
                <br>
                Two Parallelization Strategies Parallelizing Operators
<!--                : <strong>Concurrent Execution</strong> and <strong>Operator Merge</strong>-->
            </p>
            IOS consider two strategies to parallelize operators in a stage S': concurrent execution and operator merge.
            <ul>
                <li><strong>Concurrent Execution: </strong>We can launch all kernels concurrently on the accelerator (e.g., GPU).</li>
                <li><strong>Operator Merge: </strong>If we can find a single kernel to do all computation of operators in the stage, we can directly use this kernel.</li>
            </ul>
            IOS is a profile-based scheduler and would measure the execution time of both strategies for each proposed stage and select the better one.
            <!--            <h2>Transition Graph and Time Complexity</h2>-->
            <!--            To come soon.-->
            <!--            <h2>Inter-Operator Scheduler (IOS)</h2>-->

            <!--            <p>Our paper presents a hardware-efficient primitive for 3D deep learning:</p>-->
            <!--            <p style="text-align: center"><img src="figures/overview.png" style="max-width:75%"></p>-->
            <!--            <ol>-->
            <!--                <li>Its point-based branch keeps the input in a high resolution.</li>-->
            <!--                <li>Its voxel-based branch conducts the convolution over lower-resolution voxels to extract the neighborhood information.</li>-->
            <!--            </ol>-->

            <!--            <h2>Results on S3DIS</h2>-->
            <!--            <p style="text-align: center">-->
            <!--                <img src="figures/gif/PVCNN-livedemo-p1-1080p.gif" style="width:350px">-->
            <!--                <img src="figures/gif/PVCNN-livedemo-p2-1080p.gif" style="width:350px">-->
            <!--                <img src="figures/gif/PVCNN-livedemo-p3-1080p.gif" style="width:350px">-->
            <!--            </p>-->

            <!--            <h2>Introduction Video</h2>-->
            <!--            <center class="embed-responsive embed-responsive-16by9">-->
            <!--                <iframe class="embed-responsive-item" width="560" height="315" src="https://www.youtube.com/embed/h6zbQe5U1v4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
            <!--            </center>-->


            <!--            <h2>MIT Driverless</h2>-->
            <!--            <center class="embed-responsive embed-responsive-16by9">-->
            <!--                <iframe class="embed-responsive-item" width="560" height="315" src="https://www.youtube.com/embed/WW9paispAW0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
            <!--            </center>-->

            <h2>Citation</h2>
            <pre class="highlight">@inproceedings{ding2021ios,
    author={Yaoyao Ding and Ligeng Zhu and Zhihao Jia and Gennady Pekhimenko and Song Han},
    booktitle = {Proceedings of Machine Learning and Systems},
    title = {{IOS: Inter-Operator Scheduler for CNN Acceleration}},
    volume = {3},
    year = {2021}
}</pre>
<!--            @article{ding2021ios,-->
<!--            title={IOS: Inter-Operator Scheduler for CNN Acceleration},-->
<!--            author={Yaoyao Ding and Ligeng Zhu and Zhihao Jia and Gennady Pekhimenko and Song Han},-->
<!--            author = {Gleeson, James and Krishnan, Srivatsan and Gabel, Moshe and Janapa Reddi, Vijay and de Lara, Eyal and Pekhimenko, Gennady},-->
<!--            year={2021},-->
<!--            eprint={2011.01302},-->
<!--            archivePrefix={arXiv},-->
<!--            primaryClass={cs.LG}-->
<!--            }-->
<!--(Bibtex of mlsys is coming soon.)-->

            <p><strong>Acknowledgments</strong>: We want to thank
                <a href="https://www.linkedin.com/in/serina-xiaodan-tan-2602b48a" target="_blank" style="color:inherit">Xiaodan (Serina) Tan</a>
                for NVIDIA GPU related issues and
                constructive discussion. This project was supported by the Canada Foundation for Innovation JELF grant,
                NSERC Discovery grant, AWS Machine Learning Research Award, Facebook Faculty Research Award, MIT-IBM
                Watson AI Lab, MIT Data Science and AI Lab (DSAIL), NVIDIA, and NSF CAREER Award #1943349.</p>
            <hr />
            <div class="col">
                <p>
                    <a href="https://hanlab.mit.edu/" target="_blank" class="d-inline-block p-3">
                        <img height="50" src="figures/hanlab-logo.png" data-nothumb>
                    </a>
                    <a href="http://www.cs.toronto.edu/ecosystem/" target="_blank" class="d-inline-block p-3">
                        <img height="60" src="figures/ecosystem.png" data-nothumb>
                    </a>
                </p>
            </div>
        </div>
    </div> <!-- row -->

</div> <!-- container -->

</body>

</html>